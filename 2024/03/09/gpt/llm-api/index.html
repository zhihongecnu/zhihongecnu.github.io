<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>大模型 API 推理 | 看这一篇就够了 | 学习空间</title><meta name="author" content="Rex"><meta name="copyright" content="Rex"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="唠唠闲话 开源模型的训练微调和推理需要大量的设备算力，而参数量大的模型普通实验室根本跑不动。有需求就有发展的动力，比如有 vllm 等优化显存占用，加速推理的项目，或者 phi2 这种参数小但在某些方面不逊色的模型；而对于超大模型，比如前几天开源的 Grok，则可以通过代理站来使用，以上这些都极大降低了高校科研者的实验成本。 本篇聊聊模型推理的方案，从简单到复杂，从闭源到开源，给大家一个全面的上手">
<meta property="og:type" content="article">
<meta property="og:title" content="大模型 API 推理 | 看这一篇就够了">
<meta property="og:url" content="http://www.wzhecnu.cn/2024/03/09/gpt/llm-api/index.html">
<meta property="og:site_name" content="学习空间">
<meta property="og:description" content="唠唠闲话 开源模型的训练微调和推理需要大量的设备算力，而参数量大的模型普通实验室根本跑不动。有需求就有发展的动力，比如有 vllm 等优化显存占用，加速推理的项目，或者 phi2 这种参数小但在某些方面不逊色的模型；而对于超大模型，比如前几天开源的 Grok，则可以通过代理站来使用，以上这些都极大降低了高校科研者的实验成本。 本篇聊聊模型推理的方案，从简单到复杂，从闭源到开源，给大家一个全面的上手">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://qiniu.wzhecnu.cn/PicBed6/picgo/20240328002713.png">
<meta property="article:published_time" content="2024-03-09T15:10:01.000Z">
<meta property="article:modified_time" content="2024-03-27T16:28:06.435Z">
<meta property="article:author" content="Rex">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://qiniu.wzhecnu.cn/PicBed6/picgo/20240328002713.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://www.wzhecnu.cn/2024/03/09/gpt/llm-api/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="/css/iconfont.css"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":false,"highlightHeightLimit":300},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '大模型 API 推理 | 看这一篇就够了',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-03-28 00:28:06'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if (GLOBAL_CONFIG_SITE.isHome && /iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><script>(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();</script><link rel="stylesheet" href="APlayer.min.css"><div id="aplayer"></div><script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js" async></script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://qiniu.wzhecnu.cn/PicBed5/images_for_blogs/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">95</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">28</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://qiniu.wzhecnu.cn/PicBed6/picgo/20240328002713.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">学习空间</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">大模型 API 推理 | 看这一篇就够了</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发布于</span><time class="post-meta-date-created" datetime="2024-03-09T15:10:01.000Z" title="发布于 2024-03-09 23:10:01">2024-03-09</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-03-27T16:28:06.435Z" title="更新于 2024-03-28 00:28:06">2024-03-28</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">2.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>10分钟</span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="唠唠闲话">唠唠闲话</h2>
<p>开源模型的训练微调和推理需要大量的设备算力，而参数量大的模型普通实验室根本跑不动。有需求就有发展的动力，比如有 vllm 等优化显存占用，加速推理的项目，或者 phi2 这种参数小但在某些方面不逊色的模型；而对于超大模型，比如前几天开源的 Grok，则可以通过代理站来使用，以上这些都极大降低了高校科研者的实验成本。</p>
<p>本篇聊聊模型推理的方案，从简单到复杂，从闭源到开源，给大家一个全面的上手教程。</p>
<p>特别地，我们主要讨论通过 API 启动模型这一策略。</p>
<h3 id="为什么用大模型-API">为什么用大模型 API</h3>
<p>自 OpenAI 2023/03 发布 API 提供给开发者以来已经一年了；作为行业标杆，主流模型在发布时都会与 OpenAI 作为比较。而目前几乎所有基于大模型的前后端服务或开源项目，都支持OpenAI 的 API 接口，其自然而然也成为了一种规范。</p>
<p>使用 OpenAI 的接口来启动服务，有很多优点：</p>
<ul>
<li>只要写一份适配 OpenAI 接口的代码，就能同时兼容调用其他大模型</li>
<li>将大模型推理 API 化，让前后端开发与模型分离，在不需要使用模型时可以关闭服务，减少 GPU 占用</li>
<li>目前主流的模型服务，包括各类智能体应用，都支持 OpenAI API</li>
<li>目前已有很多开源工具，能轻松将模型接口转化为标准的 OpenAI 接口形式</li>
</ul>
<p>当然，也存在潜在缺点：一些模型并不完全按 OpenAI 风格设计，部分特性可能不生效，比如 system prompt 和 Function call。</p>
<p>OpenAI 的 API 规范不只是语言模型，也包括文生图模型、语音模型，以及 Embedding 模型等。除了模型推理，也包括模型微调等内容，详见<a target="_blank" rel="noopener" href="https://platform.openai.com/docs/api-reference">官方文档</a>。</p>
<h2 id="模型代理站">模型代理站</h2>
<p>OpenAI 官方的 API 需绑国外银行卡，且风控非常严格，使用门槛高。代理站的优点是省事，访问快，且支持开源模型。缺点是数据经过又一手，安全隐患增加一层，使用代理站更多是出于实验方便。这里贴几个站点：</p>
<table>
<thead>
<tr>
<th>代理站</th>
<th>访问地址</th>
<th>模型支持</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>Chatanywhere</td>
<td><a target="_blank" rel="noopener" href="https://github.com/chatanywhere/GPT_API_free">chatanywhere/GPT_API_free</a></td>
<td>仅 OpenAI 系列</td>
<td>速度快，价格低</td>
</tr>
<tr>
<td>沃卡</td>
<td><a target="_blank" rel="noopener" href="https://4.0.wokaai.com/about">wokaai.com</a></td>
<td>OpenAI/Claude/Gemini 等</td>
<td>基于 OneAPI 项目</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="http://monsterapi.ai">monsterapi.ai</a></td>
<td><a target="_blank" rel="noopener" href="https://monsterapi.ai/">monsterapi.ai</a></td>
<td>仅开源模型</td>
<td>支持微调，非 OpenAI 风格</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="http://Chandler.ai">Chandler.ai</a></td>
<td><a target="_blank" rel="noopener" href="https://chandler.bet">ChandlerAi</a></td>
<td>OpenAI/Claude/Grok 等</td>
<td>需国外信用卡</td>
</tr>
</tbody>
</table>
<p>除了 MonsterAPI，其他均为 OpenAI Style 的 API 服务。MonsterAPI 主要提供开源模型的微调和推理服务，支持多种模型和微调方式，极大降低科研实验的门槛。</p>
<p><img src="https://qiniu.wzhecnu.cn/PicBed6/picgo/20240323223430.png" alt="20240323223430"></p>
<h2 id="OneAPI-项目">OneAPI 项目</h2>
<p>代理站把数据又过了一手，更稳妥的方式是自己将模型转为 OpenAI style。提供这类服务的开源项目不少，比如 <a target="_blank" rel="noopener" href="https://github.com/Portkey-AI/gateway">AI GateWay</a> 或者 <a target="_blank" rel="noopener" href="https://github.com/songquanpeng/one-api">OneAPI</a>，我们主要介绍后者。OneAPI 是一个 OpenAI 接口的管理和分发系统，支持目前几乎所有的主流 API 服务，比如 Gemini，Claude，智谱，讯飞等。</p>
<p>我们先介绍闭源 API 转成 OpenAI Style 的方式，再讨论本地的开源模型如何以 OpenAI Style 启动。</p>
<h3 id="反向代理">反向代理</h3>
<p>OpenAI，Gemini，Claude 等 API 服务限制了国内 IP 访问，可以通过海外服务器的反向代理来解决。</p>
<p>以 Gemini 为例，写一份 Nginx 配置：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">   listen 80;</span><br><span class="line">   server_name your.domain.com;</span><br><span class="line">   location / &#123;</span><br><span class="line">      proxy_pass https://generativelanguage.googleapis.com;</span><br><span class="line">      proxy_set_header Host generativelanguage.googleapis.com;</span><br><span class="line">      proxy_set_header Upgrade <span class="variable">$http_upgrade</span>; <span class="comment"># required for ws!</span></span><br><span class="line">      proxy_set_header Connection <span class="variable">$http_connection</span>;</span><br><span class="line">      proxy_set_header X-Forwarded-For <span class="variable">$proxy_add_x_forwarded_for</span>;</span><br><span class="line">      proxy_set_header X-Forwarded-Proto <span class="variable">$scheme</span>;</span><br><span class="line">      <span class="comment">#proxy_set_header X-Real-IP $remote_addr; # 可填写服务器的真实 IP</span></span><br><span class="line">      proxy_http_version 1.1;</span><br><span class="line">      proxy_cache off;</span><br><span class="line">      proxy_buffering off;</span><br><span class="line">      proxy_redirect off;</span><br><span class="line">      proxy_request_buffering off;</span><br><span class="line">      proxy_ignore_client_abort on;</span><br><span class="line">      client_max_body_size 0;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在将 Gemini 导入 OneAPI 时，使用代理链接 <code>your.domain.com</code>。类似方法适用于 Claude 和 OpenAI 等服务。</p>
<p>此外，可以用这个方法将智谱模型默认的 <code>v4</code> 后缀转成 <code>v1</code>：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">server&#123;</span><br><span class="line">    listen 80;</span><br><span class="line">    server_name your.domain.com;</span><br><span class="line">    location /v1 &#123;</span><br><span class="line">        proxy_pass  https://open.bigmodel.cn/api/paas/v4;</span><br><span class="line">        ... <span class="comment"># 配置同上</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>由于智谱 API 使用 JWT 鉴权，复制账号密钥后，需生成有时效性的 token 才能等同于 OpenAI 的密钥。示例代码如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import time</span><br><span class="line">import jwt</span><br><span class="line">def generate_token(apikey: str, exp_seconds: int = 3600 * 24 * 30):</span><br><span class="line">    try:</span><br><span class="line">        <span class="built_in">id</span>, secret = apikey.split(<span class="string">&quot;.&quot;</span>)</span><br><span class="line">    except Exception as e:</span><br><span class="line">        raise Exception(<span class="string">&quot;invalid apikey&quot;</span>, e)</span><br><span class="line">    payload = &#123;</span><br><span class="line">        <span class="string">&quot;api_key&quot;</span>: <span class="built_in">id</span>,</span><br><span class="line">        <span class="string">&quot;exp&quot;</span>: int(round(time.time())) + exp_seconds,</span><br><span class="line">        <span class="string">&quot;timestamp&quot;</span>: int(round(time.time())),</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">return</span> jwt.encode(</span><br><span class="line">        payload,</span><br><span class="line">        secret,</span><br><span class="line">        algorithm=<span class="string">&quot;HS256&quot;</span>,</span><br><span class="line">        headers=&#123;<span class="string">&quot;alg&quot;</span>: <span class="string">&quot;HS256&quot;</span>, <span class="string">&quot;sign_type&quot;</span>: <span class="string">&quot;SIGN&quot;</span>&#125;,</span><br><span class="line">    )</span><br><span class="line">zhipu_api_key = <span class="string">&quot;your_api_key&quot;</span></span><br><span class="line"><span class="built_in">print</span>(generate_token(zhipu_api_key))</span><br></pre></td></tr></table></figure>
<p>生成有效期一个月的密钥，再导入到 OpenAI 风格的 API 服务中。</p>
<h3 id="OneAPI-配置">OneAPI 配置</h3>
<p>官方文档提供了 <a target="_blank" rel="noopener" href="https://github.com/songquanpeng/one-api/blob/main/docker-compose.yml">docker-compose</a> 一键部署的方式。部署后，访问 <a target="_blank" rel="noopener" href="http://localhost:3000/">http://localhost:3000/</a> 并登录。初始账号用户名为 root，密码为 123456，建议登录后修改。</p>
<p><img src="https://qiniu.wzhecnu.cn/PicBed6/picgo/20240323230852.png" alt="20240323230852"></p>
<p>下边介绍几个重要的功能。</p>
<h4 id="添加渠道">添加渠道</h4>
<p>点击添加渠道，将各类 API 服务添加到 OneAPI 中：</p>
<p><img src="https://qiniu.wzhecnu.cn/PicBed6/picgo/20240323231142.png" alt="20240323231142"></p>
<p>支持渠道包括 OpenAI，Gemini，Claude，智谱，讯飞等等，以 Gemini 为例：</p>
<p><img src="https://qiniu.wzhecnu.cn/PicBed6/picgo/20240323231603.png" alt="20240323231603"></p>
<p>内容说明：</p>
<ul>
<li>渠道名称：自定义</li>
<li>渠道 API 地址：官方 API 地址或者反向代理地址</li>
<li>用户组：这个选项是给号商用的，可以不管</li>
<li>模型：选择对应的模型，这是服务<strong>实际调用</strong>的模型</li>
<li>密钥：API 密钥</li>
<li>模型映射关系：用户<strong>调用</strong>的模型。比如 <code>&#123; &quot;gpt-3.5-turbo&quot;: &quot;gemini-pro&quot; &#125;</code>：当用户调用 <code>gpt-3.5-turbo</code> 时，实际调用的是 <code>gemini-pro</code> 模型。</li>
</ul>
<p>模型映射关系在一些场景很有帮助，比如在 LlamaIndex 使用 OpenAI 模型会检查模型名称，直接用非 GPT 系列的名称会报错。</p>
<h4 id="创建密钥">创建密钥</h4>
<p>添加渠道后，点击令牌，新建令牌，创建密钥，复制密钥。密钥作用等同于 OpenAI 的 API。</p>
<p><img src="https://qiniu.wzhecnu.cn/PicBed6/picgo/20240323233424.png" alt="20240323233424"></p>
<p>渠道存在优先级，如果同一模型有多个渠道支持，OneAPI 会按权重使用不同渠道。但 root 用户可通过 <code>-k</code> 指定渠道，比如下边添加了两个渠道：</p>
<p><img src="https://qiniu.wzhecnu.cn/PicBed6/picgo/20240323233805.png" alt="20240323233805"></p>
<p>通过 <code>sk-xxxxxx-1</code> 指定 <code>ChatAnywhere</code> 渠道，通过 <code>sk-xxxxxx-2</code> 指定 <code>bichat</code> 渠道。</p>
<p>注：你可以<strong>让每个模型只对应一个渠道</strong>，这样指定模型时，渠道也是确定的。</p>
<h4 id="分配用户">分配用户</h4>
<p>OneAPI 支持多用户管理，root 和管理员能管理模型渠道，配置额度。普通用户则在注册后，通过添加兑换码导入额度。</p>
<p><img src="https://qiniu.wzhecnu.cn/PicBed6/picgo/20240323232808.png" alt="20240323232808"></p>
<p>生成 API 的过程与前边一致。</p>
<h3 id="本地模型">本地模型</h3>
<p>前边讲了闭源 API 转成 OpenAI 风格的方式，下边介绍开源模型如何以 OpenAI Style 启动，继而也能导入到 OneAPI 统一管理。这方面的项目也很多，我们讲几个比较有代表性的。</p>
<h4 id="vllm">vllm</h4>
<p><a target="_blank" rel="noopener" href="https://blog.vllm.ai/2023/06/20/vllm.html">vLLM</a> 是去年6月推出的一个大模型推理加速框架，通过 PagedAttention 高效地管理 attention 中缓存的张量，实现了比 HuggingFace Transformers 高 24 倍的吞吐量。</p>
<p>vLLM 支持的开源模型包括 Llama，百川，千问等等，具体参考<a target="_blank" rel="noopener" href="https://docs.vllm.ai/en/latest/models/supported_models.html">官方列表</a>，此外，也支持基于这些模型架构训练或微调得到的模型，比如 Lemur：</p>
<p><img src="https://qiniu.wzhecnu.cn/PicBed6/picgo/20240324172632.png" alt="20240324172632"></p>
<p>vLLM 提供了一个 OpenAI 风格的 API 服务，比如启动百川模型</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pip install vllm</span></span><br><span class="line">python -m vllm.entrypoints.openai.api_server \</span><br><span class="line">   --model /sshfs/pretrains/baichuan-inc/Baichuan2-13B-Chat \</span><br><span class="line">   --trust-remote-code \</span><br><span class="line">   --tensor-parallel-size 2 \</span><br><span class="line">   --served-model-name baichuan \</span><br><span class="line">   --api-key sk-xxxx \</span><br><span class="line">   --port 8000</span><br></pre></td></tr></table></figure>
<p>参数说明：</p>
<ul>
<li><code>--model</code>：模型路径，本地路径需加上 <code>--trust-remote-code</code> 参数</li>
<li><code>--tensor-parallel-size</code>：模型使用的 GPU 数目</li>
<li><code>--served-model-name</code>：用户访问的模型名称</li>
<li><code>--api-key</code>：API 密钥，可不填，允许任意密钥访问</li>
<li><code>--port</code>：服务端口</li>
</ul>
<p>其他参数参看 <a target="_blank" rel="noopener" href="https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html">OpenAI Compatible Server</a>，vLLM 还支持很多其他功能，包括：<a target="_blank" rel="noopener" href="https://docs.vllm.ai/en/latest/quantization/auto_awq.html">推理量化</a>，<a target="_blank" rel="noopener" href="https://docs.vllm.ai/en/latest/models/lora.html">加载 Lora 参数</a>和<a target="_blank" rel="noopener" href="https://docs.vllm.ai/en/latest/serving/distributed_serving.html">分布式推理</a>等。</p>
<p>另外 vLLM 还支持 Docker 部署。在团队场景下，Docker 可用作公共服务，其他人可以随时开启和关闭，不需要重新配置环境，类似 Docker 项目还有 api-for-open-llm。</p>
<h4 id="DB-GPT">DB-GPT</h4>
<p><a target="_blank" rel="noopener" href="https://github.com/eosphoros-ai/DB-GPT">DB-GPT</a> 是一个开源的 AI 原生<strong>数据应用开发框架</strong>，项目旨在构建大模型领域的基础设施，其围绕知识数据提供了多种技术能力。</p>
<p>DB-GPT 提供了两类 API：</p>
<ul>
<li>模型 API：适配各种模型，统一封装成兼容 OpenAI 风格</li>
<li>服务层 API：DB-GPT 服务层的 API，比如 AWEL(智能体工作流编排)</li>
</ul>
<p>教程主要介绍前者，DB-GPT 的模型管理模块，其他功能可以参考<a target="_blank" rel="noopener" href="https://docs.dbgpt.site/docs/latest/application/advanced_tutorial/api">官方文档</a>。</p>
<p>环境配置：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/eosphoros-ai/DB-GPT.git</span><br><span class="line"><span class="built_in">cd</span> DB-GPT</span><br><span class="line">conda create -n dbgpt_env <span class="string">&quot;python=3.10&quot;</span></span><br><span class="line">conda activate dbgpt_env</span><br><span class="line"><span class="comment"># pip install transformers==4.35.0</span></span><br><span class="line">pip install -e <span class="string">&quot;.[openai]&quot;</span></span><br><span class="line">pip install -e <span class="string">&quot;.[default]&quot;</span></span><br><span class="line"><span class="comment"># pip install -U langchain-community</span></span><br></pre></td></tr></table></figure>
<p>命令行启动模型控制器：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dbgpt start controller --port 8000</span><br></pre></td></tr></table></figure>
<p>启动后，通过脚本将各类模型导入到控制器，举两个例子：</p>
<p>启动本地模型：比如 ChatGLM 模型</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">dbgpt start worker --model_name chatglm3-6b \</span><br><span class="line">    --model_path /sshfs/pretrains/THUDM/chatglm3-6b \</span><br><span class="line">    --port 8001 \</span><br><span class="line">    --load_4bit \</span><br><span class="line">    --num_gpus 2 \</span><br><span class="line">    --controller_addr http://localhost:8000</span><br></pre></td></tr></table></figure>
<p><code>--model_path</code> 指定模型路径，<code>model_name</code> 为用户访问的模型，<code>--load_4bit</code> 加载 4bit 模型，<code>--num_gpus</code> 指定 GPU 数目，<code>--controller_addr</code> 指定控制器地址。</p>
<p>启动 API 模型：比如智谱</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">dbgpt start worker --model_name zhipu_proxyllm \</span><br><span class="line">    --model_path zhipu_proxyllm \</span><br><span class="line">    --worker_type llm \</span><br><span class="line">    --port 8004 \</span><br><span class="line">    --proxy_server_url https://open.bigmodel.cn/api/paas/v4/chat/completions \</span><br><span class="line">    --proxy_api_key <span class="variable">$ZHIPU_API_KEY</span> \</span><br><span class="line">    --proxyllm_backend glm-4 \</span><br><span class="line">    --controller_addr http://localhost:8000</span><br></pre></td></tr></table></figure>
<p><code>--proxy_server_url</code> 指定对话链接，<code>--proxy_api_key</code> 填写智谱的密钥，<code>--proxyllm_backend</code> 指定后端实际模型，<code>--model_name</code> 为用户访问的模型。此处用官网密钥即可，不需要生成鉴权 Token，DB-GPT 会自动处理。</p>
<p>渠道添加后，启动 API 服务：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dbgpt start apiserver --api_keys sk-xxx --controller_addr http://localhost:8000</span><br></pre></td></tr></table></figure>
<p>其中 <code>--api_keys</code> 填写自定义 API 密钥，如果设置有多个可以用逗号分隔；如果不填写，则允许任何密钥访问。</p>
<h4 id="flask">flask</h4>
<p>为了更灵活的定制，可以用 flask 手写 API 服务，比如启动本地 Embedding 模型：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask, request, jsonify</span><br><span class="line"><span class="keyword">from</span> sentence_transformers <span class="keyword">import</span> SentenceTransformer</span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载模型</span></span><br><span class="line">model_path = <span class="string">&#x27;/sshfs/pretrains/moka-ai/m3e-large&#x27;</span></span><br><span class="line">model = SentenceTransformer(model_path)</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/v1/embeddings&#x27;</span>, methods=[<span class="string">&#x27;POST&#x27;</span>]</span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_embeddings</span>():</span><br><span class="line">    data = request.get_json()</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> data <span class="keyword">or</span> <span class="string">&#x27;input&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> data <span class="keyword">or</span> <span class="string">&#x27;model&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> data:</span><br><span class="line">        <span class="keyword">return</span> jsonify(&#123;<span class="string">&quot;error&quot;</span>: <span class="string">&quot;Missing required fields in request body&quot;</span>&#125;), <span class="number">400</span></span><br><span class="line">    </span><br><span class="line">    input_text = data[<span class="string">&#x27;input&#x27;</span>]</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(input_text, <span class="built_in">str</span>):</span><br><span class="line">        input_text = [input_text]  <span class="comment"># 将单个输入转换为列表</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 调用模型获取 embeddings</span></span><br><span class="line">    embeddings = model.encode(input_text)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 构造响应</span></span><br><span class="line">    embeddings_response = []</span><br><span class="line">    <span class="keyword">for</span> index, embedding <span class="keyword">in</span> <span class="built_in">enumerate</span>(embeddings):</span><br><span class="line">        embedding_dict = &#123;</span><br><span class="line">            <span class="string">&quot;object&quot;</span>: <span class="string">&quot;embedding&quot;</span>,</span><br><span class="line">            <span class="string">&quot;embedding&quot;</span>: embedding.tolist(),  <span class="comment"># 将 numpy 数组转换为列表</span></span><br><span class="line">            <span class="string">&quot;index&quot;</span>: index</span><br><span class="line">        &#125;</span><br><span class="line">        embeddings_response.append(embedding_dict)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Token 计数</span></span><br><span class="line">    response = &#123;</span><br><span class="line">        <span class="string">&quot;object&quot;</span>: <span class="string">&quot;list&quot;</span>,</span><br><span class="line">        <span class="string">&quot;data&quot;</span>: embeddings_response,</span><br><span class="line">        <span class="string">&quot;model&quot;</span>: data[<span class="string">&#x27;model&#x27;</span>],</span><br><span class="line">        <span class="string">&quot;usage&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;prompt_tokens&quot;</span>: <span class="built_in">sum</span>(<span class="built_in">len</span>(text.split()) <span class="keyword">for</span> text <span class="keyword">in</span> input_text), </span><br><span class="line">            <span class="string">&quot;total_tokens&quot;</span>: <span class="built_in">sum</span>(<span class="built_in">len</span>(text.split()) <span class="keyword">for</span> text <span class="keyword">in</span> input_text)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> jsonify(response)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    app.run(debug=<span class="literal">True</span>, port=<span class="number">5000</span>, host=<span class="string">&#x27;0.0.0.0&#x27;</span>)</span><br></pre></td></tr></table></figure>
<hr>
<p>以上，将模型接口转成 OpenAI 风格，然后再将这些接口整合到一个统一的 API 网关中，这样就可以实现一个统一的接口，供前端调用。这些是相对可用的策略，只能解决部分场景。</p>
<p>代理站解决科研人的算力资源的痛点，对技术也会相对滞后，要跟进前沿还是得有设备。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Rex</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://www.wzhecnu.cn/2024/03/09/gpt/llm-api/">http://www.wzhecnu.cn/2024/03/09/gpt/llm-api/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">文章采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank">CC BY-NC-SA 4.0</a> 许可协议，转载请注明来自 <a href="http://www.wzhecnu.cn" target="_blank">学习空间</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://qiniu.wzhecnu.cn/PicBed6/picgo/20240328002713.png" data-sites="weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button button--animated"><i class="fas fa-qrcode"></i> 请作者喝茶</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.jpg" target="_blank"><img class="post-qr-code-img" src="/img/wechat.jpg" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src="/img/alipay.jpg" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/03/12/docs-blog-tutorial/"><img class="prev-cover" src="https://qiniu.wzhecnu.cn/PicBed5/images_for_blogs/zhihu.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">docs-blog-tutorial</div></div></a></div><div class="next-post pull-right"><a href="/2024/03/02/server/ruffle-swf-flash/"><img class="next-cover" src="https://qiniu.wzhecnu.cn/PicBed6/picgo/20240303223919.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">重燃 Flash 游戏 | Ruffle 使用指南</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://qiniu.wzhecnu.cn/PicBed5/images_for_blogs/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Rex</div><div class="author-info__description">Math + Computer = ?</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">95</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">28</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/RexWzh"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/RexWzh" target="_blank" title="Github"><i class="iconfont icon-github"></i></a><a class="social-icon" href="https://space.bilibili.com/518870168" target="_blank" title="B站"><i class="iconfont icon--bilibili"></i></a><a class="social-icon" href="https://www.zhihu.com/people/wang-zhi-hong-79-21" target="_blank" title="知乎"><i class="iconfont icon-zhihu"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">新功能陆续上线，欢迎交流！干杯！！！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%94%A0%E5%94%A0%E9%97%B2%E8%AF%9D"><span class="toc-number">1.</span> <span class="toc-text">唠唠闲话</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E7%94%A8%E5%A4%A7%E6%A8%A1%E5%9E%8B-API"><span class="toc-number">1.1.</span> <span class="toc-text">为什么用大模型 API</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%90%86%E7%AB%99"><span class="toc-number">2.</span> <span class="toc-text">模型代理站</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#OneAPI-%E9%A1%B9%E7%9B%AE"><span class="toc-number">3.</span> <span class="toc-text">OneAPI 项目</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86"><span class="toc-number">3.1.</span> <span class="toc-text">反向代理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#OneAPI-%E9%85%8D%E7%BD%AE"><span class="toc-number">3.2.</span> <span class="toc-text">OneAPI 配置</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B7%BB%E5%8A%A0%E6%B8%A0%E9%81%93"><span class="toc-number">3.2.1.</span> <span class="toc-text">添加渠道</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E5%AF%86%E9%92%A5"><span class="toc-number">3.2.2.</span> <span class="toc-text">创建密钥</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%86%E9%85%8D%E7%94%A8%E6%88%B7"><span class="toc-number">3.2.3.</span> <span class="toc-text">分配用户</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%AC%E5%9C%B0%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.3.</span> <span class="toc-text">本地模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#vllm"><span class="toc-number">3.3.1.</span> <span class="toc-text">vllm</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#DB-GPT"><span class="toc-number">3.3.2.</span> <span class="toc-text">DB-GPT</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#flask"><span class="toc-number">3.3.3.</span> <span class="toc-text">flask</span></a></li></ol></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By Rex</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a><br>
<a href="https://beian.miit.gov.cn/"  style="color:#f72b07" target="_blank">粤ICP备2021109780号</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>(()=>{
  const $countDom = document.getElementById('twikoo-count')
  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'https://twikoo.wzhecnu.cn',
      region: ''
    }, null))
  }

  const getCount = () => {
    twikoo.getCommentsCount({
      envId: 'https://twikoo.wzhecnu.cn',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      $countDom.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const loadTwikoo = (bool = false) => {
    if (typeof twikoo === 'object') {
      init()
      bool && $countDom && setTimeout(getCount,0)
    } else {
      getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(()=> {
        init()
        bool && $countDom && setTimeout(getCount,0)
      })
    }
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo(true)
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="false"></script><script defer="defer" id="fluttering_ribbon" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-fluttering-ribbon.min.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-nest.min.js"></script><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/click-show-text.min.js" data-mobile="true" data-text="欢,迎,来,访,你,好,朋,友" data-fontsize="15px" data-random="false" async="async"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>